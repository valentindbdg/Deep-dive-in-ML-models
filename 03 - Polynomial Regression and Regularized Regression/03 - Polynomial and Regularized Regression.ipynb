{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression and Regularized Regression Exercise\n",
    "\n",
    "**In this exercise you will:**\n",
    "- Extend your Linear Regression model to produce non-linear regression lines/curve (more specifically polynomials)\n",
    "- Include Regularization to reduce over-fitting. More specifically, Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "# Fix the seed of the random number \n",
    "# generator so that your results will match ours\n",
    "np.random.seed(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "For this notebook, we will use a toy dataset generated from a sin function so we know exactly what the optimal function should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some sample data points\n",
    "np.random.seed(1)\n",
    "pts = 20\n",
    "over_x = np.expand_dims(np.random.uniform(-1.0,1.0, pts),1)\n",
    "over_y = np.sin(6*over_x) + np.random.randn(pts,1)*0.5\n",
    "\n",
    "plt.scatter(over_x,over_y)\n",
    "\n",
    "# plotting the optimal model\n",
    "x_range_optimal = np.arange(-1,1,0.01)\n",
    "y_range_optimal = np.sin(6*x_range_optimal)\n",
    "\n",
    "plt.plot(x_range_optimal, y_range_optimal, 'k--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The black dotted line is the function $f(x)=sin(6x)$ that generated the red dots (after some additional errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Take note that in real world scenarios, we are talking about hundreds to millions of features. So, the ability to \"look at the data and just draw the curve that feels right\" is a luxury we can enjoy now with this small dataset. In real world data, this is not the case, and we most likely cannot visualize all features altogether in one graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that a line, no matter which way you change the slope to another direction, will still not do a great job. This is because we are forcing a line to model a sin function. We call this <span style=\"color:blue\">**underfitting**</span> because we are not fitting the model to the data well. We also say that this model has <span style=\"color:blue\">**high bias**</span>, because our estimate is far off the actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "One way to remedy this is to increase the complexity of our hypothesis function through transformation. With polynomial feature transformation, we can generate more complex functions, and get a better fit of our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing!\n",
    "**Open `regularized_regression.py` and implement `poly_feature_transform`. Not only does it append a vector of ones for the bias term, it also introduces higher order polynomial terms depending on the parameter `poly_order`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from regularized_regression import RegularizedRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor = RegularizedRegression()\n",
    "np.random.seed(1)\n",
    "dummy_X = np.random.randn(5,3)\n",
    "dummy_transformed_X_1 = regressor.poly_feature_transform(dummy_X, poly_order=1)\n",
    "dummy_transformed_X_2 = regressor.poly_feature_transform(dummy_X, poly_order=2)\n",
    "dummy_transformed_X_3 = regressor.poly_feature_transform(dummy_X, poly_order=3)\n",
    "\n",
    "print(\"Before feature transform:\")\n",
    "print(dummy_X)\n",
    "print(\"Shape:\",dummy_X.shape)\n",
    "print()\n",
    "print(\"After feature transform (poly order = 1):\")\n",
    "print(dummy_transformed_X_1)\n",
    "print(\"Shape:\",dummy_transformed_X_1.shape)\n",
    "print()\n",
    "print(\"After feature transform (poly order = 2):\")\n",
    "print(dummy_transformed_X_2)\n",
    "print(\"Shape:\",dummy_transformed_X_2.shape)\n",
    "print()\n",
    "print(\"After feature transform (poly order = 3):\")\n",
    "print(dummy_transformed_X_3)\n",
    "print(\"Shape:\",dummy_transformed_X_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output of the polynomial feature transform function.\n",
    "\n",
    "```\n",
    "Before feature transform:\n",
    "[[ 1.62434536 -0.61175641 -0.52817175]\n",
    " [-1.07296862  0.86540763 -2.3015387 ]\n",
    " [ 1.74481176 -0.7612069   0.3190391 ]\n",
    " [-0.24937038  1.46210794 -2.06014071]\n",
    " [-0.3224172  -0.38405435  1.13376944]]\n",
    "Shape: (5, 3)\n",
    "\n",
    "After feature transform (poly order = 1):\n",
    "[[ 1.62434536 -0.61175641 -0.52817175  1.        ]\n",
    " [-1.07296862  0.86540763 -2.3015387   1.        ]\n",
    " [ 1.74481176 -0.7612069   0.3190391   1.        ]\n",
    " [-0.24937038  1.46210794 -2.06014071  1.        ]\n",
    " [-0.3224172  -0.38405435  1.13376944  1.        ]]\n",
    "Shape: (5, 4)\n",
    "\n",
    "After feature transform (poly order = 2):\n",
    "[[ 1.62434536 -0.61175641 -0.52817175  2.63849786  0.37424591  0.2789654\n",
    "   1.        ]\n",
    " [-1.07296862  0.86540763 -2.3015387   1.15126166  0.74893036  5.29708037\n",
    "   1.        ]\n",
    " [ 1.74481176 -0.7612069   0.3190391   3.04436809  0.57943595  0.10178594\n",
    "   1.        ]\n",
    " [-0.24937038  1.46210794 -2.06014071  0.06218558  2.13775962  4.24417974\n",
    "   1.        ]\n",
    " [-0.3224172  -0.38405435  1.13376944  0.10395285  0.14749775  1.28543315\n",
    "   1.        ]]\n",
    "Shape: (5, 7)\n",
    "\n",
    "After feature transform (poly order = 3):\n",
    "[[  1.62434536  -0.61175641  -0.52817175   2.63849786   0.37424591\n",
    "    0.2789654    4.28583177  -0.22894734  -0.14734164   1.        ]\n",
    " [ -1.07296862   0.86540763  -2.3015387    1.15126166   0.74893036\n",
    "    5.29708037  -1.23526764   0.64813005 -12.19143546   1.        ]\n",
    " [  1.74481176  -0.7612069    0.3190391    3.04436809   0.57943595\n",
    "    0.10178594   5.31184926  -0.44107064   0.0324737    1.        ]\n",
    " [ -0.24937038   1.46210794  -2.06014071   0.06218558   2.13775962\n",
    "    4.24417974  -0.01550724   3.12563531  -8.74360747   1.        ]\n",
    " [ -0.3224172   -0.38405435   1.13376944   0.10395285   0.14749775\n",
    "    1.28543315  -0.03351619  -0.05664715   1.45738482   1.        ]]\n",
    "Shape: (5, 10)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train! Compute for the weights via closed form/analytical solution\n",
    "**In `regularized_regression.py`, fill in the code for the function `train_analytic`, and `predict`. For now, ignore the `lambda_reg` parameter, we will incorporate that later on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor = RegularizedRegression()\n",
    "regressor.train_analytic(over_x,over_y,poly_order=3)\n",
    "X_range = np.expand_dims(np.arange(-1.0,1.0,2/100),1)\n",
    "y_range = regressor.predict(X_range)\n",
    "plt.scatter(over_x,over_y)\n",
    "plt.plot(X_range,y_range, \"b\")\n",
    "plt.ylim(-2,2)\n",
    "plt.title(\"Order = 3\")\n",
    "\n",
    "print(\"Weight vector:\")\n",
    "print(regressor.params['W'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected weights $W$.\n",
    "\n",
    "```\n",
    "Weight vector:\n",
    "[[ 0.84215997]\n",
    " [-0.96338663]\n",
    " [-2.81534817]\n",
    " [ 0.11406516]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the our hypothesis function is now showing cubic function. Let's see the effect of increasing the complexity of our hypothesis function; i.e., increasing the polynomial order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regressor = RegularizedRegression()\n",
    "\n",
    "weights={}\n",
    "plt.figure(figsize=(15,12))\n",
    "for i in range(9):\n",
    "    order = 2*i + 1\n",
    "    regressor.train_analytic(over_x,over_y,poly_order=order)\n",
    "\n",
    "    X_range = np.expand_dims(np.arange(-1.0,1.0,2/100),1)\n",
    "    y_range = regressor.predict(X_range)\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.scatter(over_x,over_y)\n",
    "    plt.plot(X_range,y_range, \"b\")\n",
    "    plt.ylim(-2,2)\n",
    "    plt.title(\"Order = \" + str(order))\n",
    "    \n",
    "    weights[order] = regressor.params['W']\n",
    "    plt.plot(x_range_optimal, y_range_optimal, 'k--',alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the generated models with higher orders. It definitely got a lot of data correct, but we know that this is not the sin wave function where the data actually came from.\n",
    "\n",
    "You may also notice that it dipped/peaked into some points that are unneccessary. It has freedom to this as long as the data it wants to target are met.\n",
    "\n",
    "We call this <span style=\"color:blue\">**overfitting**</span>, because while it will work well with this particular set of data, it might do badly with other unseen data points. Overfitting gives us a low training error (good!), but a high test error (bad!). This is because it didn't generalize well.\n",
    "\n",
    "So even if it does model our data well with a higher degree, our data is also inherently noisy which causes the model to look this way. With that said, a more complex function will fit the noise better as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    idx = 2*i + 1\n",
    "    length = weights[idx].shape[0]\n",
    "    curweights = weights[idx]\n",
    "    print(\"order =\", idx)\n",
    "    for weight in curweights:\n",
    "        print(str(weight),end=\"\\t\")\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the weights of the model with polynomial order = 17 are orders of magnitude larger than the model with polynomial order = 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try it on the gradient descent solution.\n",
    "\n",
    "**In `regularized_regression.py`, implement the following functions:**\n",
    "- `initialize_weights`\n",
    "- `loss`\n",
    "- `train`\n",
    "\n",
    "It should be the same with the previous assignment on Linear Regression\n",
    "\n",
    "**Note: Ignore the the parameter lambda_reg for now, we will include it later on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor = RegularizedRegression()\n",
    "np.random.seed(1)\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "loss_history = regressor.train(over_x,over_y,poly_order=17,learning_rate=1.1,num_iters=10000)\n",
    "\n",
    "X_range = np.expand_dims(np.arange(-1.0,1.0,2/100),1)\n",
    "y_range = regressor.predict(X_range)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(over_x,over_y)\n",
    "plt.plot(X_range,y_range, \"b\")\n",
    "plt.ylim(-2,2)\n",
    "plt.title(\"Order = 17\")\n",
    "    \n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(loss_history)\n",
    "\n",
    "print(\"Weight vector:\")\n",
    "print(regressor.params['W'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected weights $W$.\n",
    "\n",
    "```\n",
    "Weight vector:\n",
    "[[  3.66210965]\n",
    " [ -2.82887195]\n",
    " [-13.29268421]\n",
    " [  3.80986767]\n",
    " [  1.71515421]\n",
    " [  0.8398426 ]\n",
    " [  5.35130437]\n",
    " [ -1.1191464 ]\n",
    " [  4.81923714]\n",
    " [ -1.3389015 ]\n",
    " [  3.08785639]\n",
    " [ -0.49759083]\n",
    " [  1.02425301]\n",
    " [  0.92366167]\n",
    " [ -1.02343876]\n",
    " [  2.55520453]\n",
    " [ -3.01965823]\n",
    " [  0.18169391]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How come the solution is still well behaved? \n",
    "\n",
    "If you will look at the weights of the analytic solution at order = 17, some of the magnitudes are in the order of $10^5$. We are initializing our weights to be around $10^{-2}$, and every update is a small step towards the optimal solution. It will take a very long time for gradient descent to reach the same answer as the analytic solution. If you want to know more about this behavior search for literature on *implicit regularization*. You can try to modify the weights initialization to a much larger value and you will see that the functions will start to be wilder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Regression\n",
    "\n",
    "As the order of our hypothesis function increases, we are able to generate more complex functions and get a better fit of our dataset. However, our data is inherently noisy and more complex functions fit noise better as well. \n",
    "\n",
    "To prevent overfitting, we add a regularization term in our loss function that penalizes large weights. \n",
    "\n",
    "**Modify your implementation of `train_analytical` function to include $L_2$ regularization.**\n",
    "\n",
    "Hint: $(X^TX)^{-1} => (X^TX + \\lambda I)^{-1}$ where $\\lambda$ is the regularization coefficient that controls how much you penalize large weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor = RegularizedRegression()\n",
    "    \n",
    "weights={}\n",
    "    \n",
    "plt.figure(figsize=(15,12))\n",
    "for i in range(9):\n",
    "    order = 2*i + 1\n",
    "    regressor.train_analytic(over_x,over_y,poly_order=order,lambda_reg=0.05)\n",
    "\n",
    "    X_range = np.expand_dims(np.arange(-1.0,1.0,2/100),1)\n",
    "    y_range = regressor.predict(X_range)\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.scatter(over_x,over_y)\n",
    "    plt.plot(X_range,y_range, \"b\")\n",
    "    plt.ylim(-2,2)\n",
    "    plt.title(\"Order = \" + str(order))\n",
    "    weights[order] = regressor.params['W']\n",
    "\n",
    "    plt.plot(x_range_optimal, y_range_optimal, 'k--',alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    idx = 2*i + 1\n",
    "    length = weights[idx].shape[0]\n",
    "    curweights = weights[idx]\n",
    "    print(\"order =\", idx)\n",
    "    for weight in curweights:\n",
    "        print(str(weight),end=\"\\t\")\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "```\n",
    "order = 1\n",
    "[-0.4486202]\t[-0.05937566]\t\n",
    "\n",
    "order = 3\n",
    "[ 0.6235059]\t[-0.71939875]\t[-2.30221007]\t[ 0.06582113]\t\n",
    "\n",
    "order = 5\n",
    "[ 1.24112442]\t[ 1.03232305]\t[-4.39945598]\t[-2.68683672]\t[ 0.84027246]\t[-0.06107957]\t\n",
    "\n",
    "order = 7\n",
    "[ 1.52356038]\t[ 0.80065646]\t[-4.53217155]\t[-0.76149416]\t[-1.08317214]\t[-2.34360019]\t[ 1.36797296]\t[-0.07387634]\t\n",
    "\n",
    "order = 9\n",
    "[ 1.61300761]\t[ 0.54660529]\t[-4.33625202]\t[-0.28079276]\t[-1.64597931]\t[-1.25016518]\t[ 0.18536439]\t[-1.79275075]\t[ 1.15537026]\t[-0.06567512]\t\n",
    "\n",
    "order = 11\n",
    "[ 1.62878277]\t[ 0.42955622]\t[-4.17273986]\t[-0.20792035]\t[-1.76695785]\t[-0.90851918]\t[-0.23039595]\t[-1.2083724]\t[ 0.49655744]\t[-1.23665026]\t[ 0.76311008]\t[-0.05876149]\t\n",
    "\n",
    "order = 13\n",
    "[ 1.62485869]\t[ 0.39489285]\t[-4.08928028]\t[-0.22451957]\t[-1.76705735]\t[-0.82956633]\t[-0.34713495]\t[-1.02262311]\t[ 0.26974652]\t[-0.95341296]\t[ 0.44154774]\t[-0.75019763]\t[ 0.3829582]\t[-0.05561562]\t\n",
    "\n",
    "order = 15\n",
    "[ 1.62112146]\t[ 0.39075192]\t[-4.06252472]\t[-0.23832257]\t[-1.75477919]\t[-0.8216589]\t[-0.36585905]\t[-0.98393614]\t[ 0.21776962]\t[-0.88377457]\t[ 0.35875931]\t[-0.65228379]\t[ 0.27293439]\t[-0.36477232]\t[ 0.07650277]\t[-0.05483464]\t\n",
    "\n",
    "order = 17\n",
    "[ 1.62209274]\t[ 0.39083754]\t[-4.06393028]\t[-0.233149]\t[-1.75742168]\t[-0.81818179]\t[-0.3659769]\t[-0.98471801]\t[ 0.22181705]\t[-0.88949343]\t[ 0.36742106]\t[-0.66288098]\t[ 0.286126]\t[-0.3799179]\t[ 0.09392481]\t[-0.080347]\t[-0.14692737]\t[-0.05498183]\t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the learned models are more well behaved even at higher orders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the gradient descent approach.\n",
    "\n",
    "**Modify your implementation of `loss` function to include $L_2$ regularization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor = RegularizedRegression()\n",
    "np.random.seed(1)\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "loss_history = regressor.train(over_x,over_y,poly_order=17,lambda_reg=0.005, learning_rate=1.1, num_iters=10000)\n",
    "\n",
    "X_range = np.expand_dims(np.arange(-1.0,1.0,2/100),1)\n",
    "y_range = regressor.predict(X_range)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(over_x,over_y)\n",
    "plt.plot(X_range,y_range, \"b\")\n",
    "plt.ylim(-2,2)\n",
    "plt.title(\"Order = 17, lambda = 0.005\")\n",
    "    \n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss History')\n",
    "\n",
    "print(\"Weight vector:\")\n",
    "print(regressor.params['W'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "```\n",
    "Weight vector:\n",
    "[[ 1.16909827]\n",
    " [ 0.28496586]\n",
    " [-3.02590419]\n",
    " [-0.20696395]\n",
    " [-1.5283569 ]\n",
    " [-0.55036286]\n",
    " [-0.49987679]\n",
    " [-0.65717011]\n",
    " [ 0.01085529]\n",
    " [-0.62759877]\n",
    " [ 0.21684787]\n",
    " [-0.52959185]\n",
    " [ 0.25758976]\n",
    " [-0.4008975 ]\n",
    " [ 0.21171751]\n",
    " [-0.26155844]\n",
    " [ 0.12279446]\n",
    " [-0.05889166]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is not that obvious for $\\lambda=0.005$, let us try to vary it and see how it affects the learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor = RegularizedRegression()\n",
    "np.random.seed(1)\n",
    "lambda_reg_list = [0.0001,0.001, 0.005, 0.01, 0.03, 0.05]\n",
    "plt_ctr = 1\n",
    "plt.figure(figsize=(18,13))\n",
    "for reg in lambda_reg_list:\n",
    "\n",
    "    loss_history = regressor.train(over_x,over_y,poly_order=17,lambda_reg=reg, learning_rate=1.1, num_iters=10000)\n",
    "\n",
    "    X_range = np.expand_dims(np.arange(-1.0,1.0,2/100),1)\n",
    "    y_range = regressor.predict(X_range)\n",
    "    plt.subplot(3,4,plt_ctr)\n",
    "    plt.scatter(over_x,over_y)\n",
    "    plt.plot(X_range,y_range, \"b\")\n",
    "    plt.ylim(-2,2)\n",
    "    plt.title(\"lambda = \"+str(reg))\n",
    "    plt_ctr += 1\n",
    "    plt.subplot(3,4,plt_ctr)\n",
    "    plt.plot(loss_history)\n",
    "    plt.title('Loss History')\n",
    "    plt_ctr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the regression curve becomes flatter as the regularization strength increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Another way of preventing overfitting\n",
    "\n",
    "In class, we discussed that having more data points will prevent overfitting. Let's see what solutions we'll end up with if we have much more data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pts = 1000\n",
    "over_x_many = np.expand_dims(np.random.uniform(-1.0,1.0, pts),1)\n",
    "over_y_many = np.sin(6*over_x_many) + np.random.randn(pts,1)*0.5\n",
    "\n",
    "plt.scatter(over_x_many,over_y_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regressor = RegularizedRegression()\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "for i in range(9):\n",
    "    order = 2*i + 1\n",
    "    regressor.train_analytic(over_x_many,over_y_many,poly_order=order,lambda_reg=0)\n",
    "\n",
    "    X_range = np.expand_dims(np.arange(-1.0,1.0,2/100),1)\n",
    "    y_range = regressor.predict(X_range)\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.scatter(over_x_many,over_y_many)\n",
    "    plt.plot(X_range,y_range,\"b\")\n",
    "    plt.ylim(-2,2)\n",
    "    plt.title(\"Order = \" + str(order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, even without regularization, the more data we have, our model is more consistent and less likely to overfit. This suggests that overfitting is relative to the **complexity** of the hypothesis function as well as the **number of data points** in your training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Congratulations!\n",
    "\n",
    "You have now created a linear regression model that can fit a complex function that is robust to the problem of overfitting. \n",
    "\n",
    "<img src=\"http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png\">\n",
    "In the figure above, we want to be where the error is the lowest\n",
    "\n",
    "To **summarize**:\n",
    "* We definitely do not want to underfit, so we allow feature transform to properly capture data\n",
    "* But we don't want to overfit either by doing feature transform, so we must make sure that we don't overfit by keeping an eye on the weights.\n",
    "* We do this by updating our loss function to also consider large weights (weights far from zero) as additional loss\n",
    "* Weights now update to make sure that training error is minimal and weights be as close to 0 as possible.\n",
    "\n",
    "\n",
    "\n",
    "Next, we will move on to logistic regression, which builds on top of linear regression. Unlike linear regression, logistic regression is a classification technique.\n",
    "\n",
    "See you in the next notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>fin</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
