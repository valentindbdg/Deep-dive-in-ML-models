{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Exercise\n",
    "\n",
    "This exercise will guide you in implementing a Linear Model for classification namely, Logistic Regression, to gain intuitions and develop a deeper understanding of classification models. These concepts will form as the foundation for more complex models later on.\n",
    "\n",
    "**You will learn to:**\n",
    "- Build the general architecture of a Logistic Regression Model.\n",
    "    - Initializing Parameters/Weights\n",
    "    - Implement the activation function that maps your raw scores to probabilities.\n",
    "    - Calculating the Cost/Loss/Objective Function\n",
    "    - Computing for the gradients of the Loss function with respect to the parameters\n",
    "    - Implement gradient descent to update the paramters\n",
    "- Write data into an hdf5 file\n",
    "    - store (multiple) data sets in one hdf5 file\n",
    "- Read data from an hdf5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Fix the seed of the random number \n",
    "# generator so that your results will match ours\n",
    "np.random.seed(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Let's first start with a toy dataset, so we can visualize our data in 2D. The data generated below are sampled from two Gaussian distributions centered at $(-3,0)$ and $(-1,4)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "from sklearn.datasets import make_blobs\n",
    "centers = [[-3, 0], [-1, 4]]\n",
    "\n",
    "n_samples = 1500\n",
    "X, y = make_blobs(n_samples=n_samples, centers=centers)\n",
    "y = np.expand_dims(y,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "plt.scatter(X[:,0], X[:,1],c=np.squeeze(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X \\in \\mathbb{R}^{N,D}$ - our data is represented as a matrix with $N$ rows and $D$ columns, where each row is a $D$-dimensional feature vector representing an instance / example in our dataset $(x_i \\in \\mathbb{R}^D)$. In this particular example, $D=2$.\n",
    "\n",
    "\n",
    "\n",
    "$y \\in \\{1,0\\}^N$ - the prediction target is represented as a vector of length $N$ and each example can only take on either $1$ or $0$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"X[{}] = {}\\t y[{}] = {}\".format(i, X[i], i, y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Initialize Weights! We initialize the weights with small random values and the biases are initialized to zero.\n",
    "**Open `logistic_regression.py`, and fill in the code for the function `initialize_weights`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from logistic_regression import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "classifier = LogisticRegression()\n",
    "classifier.initialize_weights(5)\n",
    "print(\"Weights vector:\")\n",
    "print(classifier.params[\"W\"])\n",
    "print(\"Bias:\")\n",
    "print(classifier.params[\"b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Weights vector:\n",
    "[[ 0.01624345]\n",
    " [-0.00611756]\n",
    " [-0.00528172]\n",
    " [-0.01072969]\n",
    " [ 0.00865408]]\n",
    "Bias:\n",
    "0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute for the predictions using the current weights\n",
    "**To do that, we must first implement the `sigmoid` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "np.random.seed(1)\n",
    "print(classifier.sigmoid(np.random.randn(5,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "```\n",
    "[[ 0.83539354  0.35165864  0.3709434   0.25483894  0.70378922]\n",
    " [ 0.09099561  0.85129722  0.31838429  0.57909005  0.43797848]\n",
    " [ 0.81185487  0.11303172  0.42008677  0.40514941  0.75653387]\n",
    " [ 0.24976027  0.45699943  0.29362176  0.51055187  0.64171493]\n",
    " [ 0.2496239   0.75854586  0.71127629  0.62304533  0.71112537]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, implement the `predict` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "np.random.seed(1)\n",
    "classifier.initialize_weights(5)\n",
    "print(\"Predictions\")\n",
    "print(classifier.predict(np.random.randn(6,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Predictions\n",
    "[[0]\n",
    " [1]\n",
    " [0]\n",
    " [0]\n",
    " [1]\n",
    " [0]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then compute for the losses of the predictions.\n",
    "**First implement `binary_cross_entropy` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "probs = np.random.uniform(0,1,size=(5,1))\n",
    "labels = np.expand_dims(np.array([0,1,0,1,0]),-1)\n",
    "print(\"Binary Cross Entropy loss:\",classifier.binary_cross_entropy(probs,labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Binary Cross Entropy loss: 0.444542186123\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, implement `loss` function which should output the losses and the gradients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "np.random.seed(1)\n",
    "classifier.initialize_weights(5)\n",
    "loss, grads = classifier.loss(np.random.randn(5,5),labels)\n",
    "print(\"Loss:\",loss)\n",
    "print(\"Gradient['W']\",grads['W'])\n",
    "print(\"Gradient['b']\",grads['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Loss: 0.685306262585\n",
    "Gradient['W'] [[-0.43331022]\n",
    " [ 0.22530298]\n",
    " [-0.31534154]\n",
    " [-0.00502479]\n",
    " [-0.11491087]]\n",
    "Gradient['b'] 0.0984799408615\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the gradient descent in the `train` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "classifier = LogisticRegression()\n",
    "loss_history = classifier.train(X, y, learning_rate=1e-1, num_iters=1000, batch_size=256, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "iteration 100 / 1000: loss 0.070082\n",
    "iteration 200 / 1000: loss 0.071722\n",
    "iteration 300 / 1000: loss 0.045378\n",
    "iteration 400 / 1000: loss 0.049017\n",
    "iteration 500 / 1000: loss 0.039071\n",
    "iteration 600 / 1000: loss 0.024814\n",
    "iteration 700 / 1000: loss 0.026963\n",
    "iteration 800 / 1000: loss 0.051802\n",
    "iteration 900 / 1000: loss 0.034741\n",
    "iteration 1000 / 1000: loss 0.026757\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the decision boundary.\n",
    "Hint: This is where the model is unsure about the class, (probability = 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the data\n",
    "plt.scatter(X[:,0], X[:,1],c=np.squeeze(y))\n",
    "\n",
    "W = classifier.params['W']\n",
    "b = classifier.params['b']\n",
    "\n",
    "x1_range = np.arange(-6,2,0.1)\n",
    "\n",
    "#########################################################################\n",
    "# TODO: Compute for the decision boundary of our model. Given the x1    #\n",
    "# values above what should be the values of x2?                         #\n",
    "#########################################################################\n",
    "x2_range = None\n",
    "#########################################################################\n",
    "#                             END OF YOUR CODE                          #\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "plt.plot(x1_range,x2_range,'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary should look like this.\n",
    "<img src=\"decision_boundary.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Dataset\n",
    "\n",
    "Let's try to apply logistic regression on actual images. We will be working with $64 \\times 64$ images stored as separate `png` files in the `cat_train` and `cat_test` folders. The folders also include a text file (`labels.txt`) that encodes the ground truth labels for each image. But first let's learn a better way to store these data for faster access during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = \"cat_train\"\n",
    "test_path = \"cat_test\"\n",
    "\n",
    "# get the list of file names in each folder\n",
    "train_filenames = []\n",
    "for file in os.listdir(train_path):\n",
    "    if file.endswith(\".png\"):\n",
    "        train_filenames.append(\"/\".join([train_path, file]))\n",
    "        \n",
    "test_filenames = []\n",
    "for file in os.listdir(test_path):\n",
    "    if file.endswith(\".png\"):\n",
    "        test_filenames.append(\"/\".join([test_path, file]))\n",
    "\n",
    "# read the corresponding labels for the train and test images\n",
    "train_labels = []\n",
    "with open(train_path + \"/labels.txt\",\"r\") as f:\n",
    "    for l in f:\n",
    "        train_labels.append(int(l))\n",
    "\n",
    "test_labels = []\n",
    "with open(test_path + \"/labels.txt\",\"r\") as f:\n",
    "    for l in f:\n",
    "        test_labels.append(int(l))\n",
    "        \n",
    "        \n",
    "num_train = len(train_filenames)\n",
    "num_test = len(test_filenames)\n",
    "\n",
    "print(\"Number of training images =\", num_train)\n",
    "print(\"Number of test images =\", num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "randIdx = np.arange(num_train)\n",
    "np.random.shuffle(randIdx)\n",
    "plt.figure(figsize=(6,6))\n",
    "label_names = [\"not cat\", \"cat\"]\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    img = misc.imread(train_filenames[randIdx[i]]) / 255.0\n",
    "    label = train_labels[randIdx[i]]\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(label_names[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Writing HDF5 Files\n",
    "\n",
    "Most of the time, your data will be in separate files which will be stored in separate spaces in your memory (specially true for image data). This is not efficient since we have to load the data one by one during training. A better way is to store all the data into one file so they are mapped in a contiguous space in the memory. However, this will pose another problem when your data is too big to fit in your memory. Memory mapped file systems / databases such as HDF5 addresses this problem by directly reading data from your storage and only loading the data you are currently reading into memory. It also provides a seemless interface that works as if you are working with a giant matrix. \n",
    "\n",
    "## Don't forget to install the h5py package and Pillow to read and write images\n",
    "**`pip install Pillow`**\n",
    "\n",
    "**`pip install h5py`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an hdf5 file and write data into it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a blank hdf5 file. You can think of this as a dictionary which can store data referenced/indexed by keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saveFile = h5py.File('cat_dataset.hdf5','w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_dataset function adds keys to your hdf5 file. This is similar to a python dictionary.\n",
    "\n",
    "The important parameters are:\n",
    "1. the name of the key\n",
    "2. the shape of the data that will be stored. \n",
    "    - For images it is usually in the format (N, H, W, C) where N is the number of images, H is the height in pixels, W is the width in pixels, and C is the number of channels (3 for colored images representing RGB)\n",
    "3. the data type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_height = 64\n",
    "img_width = 64\n",
    "\n",
    "# create datasets for the training images and labels\n",
    "train_images_hdf5 = saveFile.create_dataset(\"train_x\", shape=(num_train, img_height, img_width,3), dtype='float32')\n",
    "train_labels_hdf5 = saveFile.create_dataset(\"train_y\", shape=(num_train,1), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_train):\n",
    "    # read the images under `cat_train`\n",
    "    img = misc.imread(train_filenames[i])\n",
    "    # normalize images to [0,1] and store it in the hdf5 file under the \"train_x\" key/dataset.\n",
    "    train_images_hdf5[i] = img / 255. \n",
    "    train_labels_hdf5[i] = train_labels[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly read data from the hdf5 file to check if the write was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "randIdx = np.arange(num_train)\n",
    "np.random.shuffle(randIdx)\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(train_images_hdf5[randIdx[i]])\n",
    "    plt.axis('off')\n",
    "    plt.title(label_names[int(train_labels_hdf5[randIdx[i]])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it's your turn to do the same for the test images.\n",
    "Create datasets under the same hdf5 file for the testing images and labels with keys `test_x` and `test_y` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TODO: Create datasets for the testing images and labels               #\n",
    "#########################################################################\n",
    "test_images_hdf5 = None\n",
    "test_labels_hdf5 = None\n",
    "#########################################################################\n",
    "#                             END OF YOUR CODE                          #\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TODO: Load the test images and store them in the hdf5 file under the  #\n",
    "# \"test_x\" and \"test_y\" key/dataset.                                    #\n",
    "#########################################################################\n",
    "\n",
    "#########################################################################\n",
    "#                             END OF YOUR CODE                          #\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "randIdx = np.arange(num_test)\n",
    "np.random.shuffle(randIdx)\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(test_images_hdf5[randIdx[i]])\n",
    "    plt.axis('off')\n",
    "    plt.title(label_names[int(test_labels_hdf5[randIdx[i]])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any file read / write, we need to close the file writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# close the hdf5 file writer.\n",
    "saveFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's read hdf5 file that you just created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the hdf5 in read mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readHdf5 = h5py.File('cat_dataset.hdf5','r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the available keys inside the hdf5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in readHdf5.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Train images shape =\",readHdf5[\"train_x\"].shape)\n",
    "print(\"Train labels shape =\",readHdf5[\"train_y\"].shape)\n",
    "print(\"Test images shape =\",readHdf5[\"test_x\"].shape)\n",
    "print(\"Test labels shape =\",readHdf5[\"test_y\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# randomly read and show images stored in the hdf5 file\n",
    "randIdx = np.arange(readHdf5[\"train_x\"].shape[0])\n",
    "np.random.shuffle(randIdx)\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(readHdf5[\"train_x\"][randIdx[i]])\n",
    "    plt.axis('off')\n",
    "    plt.title(label_names[int(readHdf5[\"train_y\"][randIdx[i]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly read and show images stored in the hdf5 file\n",
    "randIdx = np.arange(readHdf5[\"test_x\"].shape[0])\n",
    "np.random.shuffle(randIdx)\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(readHdf5[\"test_x\"][randIdx[i]])\n",
    "    plt.axis('off')\n",
    "    plt.title(label_names[int(readHdf5[\"test_y\"][randIdx[i]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readHdf5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train a Logistic Regression model on this dataset to classify cat images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = h5py.File(\"cat_dataset.hdf5\")\n",
    "\n",
    "train_images = data[\"train_x\"]\n",
    "y_train = np.array(data[\"train_y\"])\n",
    "test_images = data[\"test_x\"]\n",
    "y_test = np.array(data[\"test_y\"])\n",
    "\n",
    "num_train, H, W, C = train_images.shape\n",
    "num_test,_,_,_ = test_images.shape\n",
    "\n",
    "print(\"Train images shape =\", train_images.shape)\n",
    "print(\"Train labels shape =\", y_train.shape)\n",
    "print(\"Test images shape =\", test_images.shape)\n",
    "print(\"Test labels shape =\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.reshape(train_images,[num_train,-1])\n",
    "X_test = np.reshape(test_images,[num_test,-1])\n",
    "\n",
    "print(\"X_train shape =\",X_train.shape)\n",
    "print(\"X_test shape =\",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from logistic_regression import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "classifier = LogisticRegression()\n",
    "loss_history = classifier.train(X_train, y_train, learning_rate=5e-3, num_iters=2000, batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_train_pred = classifier.predict(X_train)\n",
    "Y_test_pred = classifier.predict(X_test)\n",
    "print(\"Train accuracy: {} %\".format(100 - np.mean(np.abs(Y_train_pred - y_train)) * 100))\n",
    "print(\"Test accuracy: {} %\".format(100 - np.mean(np.abs(Y_test_pred - y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get similar accuracies as the ones below. As you can see there is a big gap between the train and test accuracies, this means that our model is not generalizing well on unforseen data.\n",
    "\n",
    "```\n",
    "Train accuracy: 99.04306220095694 %\n",
    "Test accuracy: 70.0 %\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = classifier.predict(X_test[4])\n",
    "plt.imshow(test_images[4])\n",
    "plt.axis('off')\n",
    "plt.title(\"y = \" + label_names[(np.squeeze(y_test[4]))] + \" | prediction =\" + label_names[np.squeeze(prediction)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
