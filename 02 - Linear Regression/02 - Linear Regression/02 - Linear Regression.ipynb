{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Exercise\n",
    "\n",
    "This exercise will guide you in implementing the Linear Regression Model to gain intuitions and develop a deeper understanding of the model. These concepts will form as the foundation for more complex models later on.\n",
    "\n",
    "**You will learn to:**\n",
    "- Build the general architecture of a Linear Regression Model.\n",
    "    - Implement the Analytical solution for the parameters that minimizes the loss function\n",
    "    - Implement the Approximate / Iterative solution for finding the parameters that minimizes the Loss function \n",
    "        - Initializing Parameters\n",
    "        - Calculating the Cost/Loss/Objective Function\n",
    "        - Computing for the gradients of the Loss function with respect to the parameters\n",
    "        - Implement gradient descent to update the paramters\n",
    "        \n",
    "**General Instructions:**\n",
    "- You may add your own variables but you may not delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "# Fix the seed of the random number \n",
    "# generator so that your results will match ours\n",
    "np.random.seed(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will use the sales prices of houses in Kansas City as our dataset. It has 21 columns:\n",
    "\n",
    "1. **id** - (numeric) unique id for each house\n",
    "2. **date** - (string) Date house was sold\n",
    "3. **price** - (numeric) Price at which the house was sold\n",
    "4. **bedrooms** - (numeric) Number of bedrooms\n",
    "5. **bathrooms** - (numeric) Number of bathrooms  / bedrooms\n",
    "6. **sqft_living** - (numeric) Square footage of the house\n",
    "7. **sqft_lot** - (numeric) Square footage of the lot\n",
    "8. **floors** - (numeric) Total floors (levels) in the house\n",
    "9. **waterfront** - (numeric) House which has a view to a waterfront\n",
    "10. **view** - (numeric) Has been viewed (binary)\n",
    "11. **condition** - (numeric) How good he condition is overall\n",
    "12. **grade** - (numeric)  Overall grade given to the housing unit, based on King County grading system\n",
    "13. **sqft_above** - (numeric) Square footage of house apart from basement\n",
    "14. **sqft_basement** - (numeric) Square footage of the basement\n",
    "15. **yr_built** - (numeric) Year the house was built\n",
    "16. **yr_renovated** - (numeric) Year when the house was renovated\n",
    "17. **zipcode** - (numeric) Zip-code\n",
    "18. **lat** - (numeric) Latitude coordinate\n",
    "19. **long** - (numeric) Longitude coordinate\n",
    "20. **sqft_living15** - (numeric) Living room area in 2015(implies-- some renovations). This might or might not have affected the lotsize area\n",
    "21. **sqft_lot15** - (numeric) LotSize area in 2015(implies-- some renovations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reads the csv file and converts it into a pandas dataframe\n",
    "rawData = pd.read_csv('kc_house_data.csv')\n",
    "print(\"Shape of the raw data\", rawData.shape)\n",
    "\n",
    "# displays the first 5 entries\n",
    "rawData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict the price of a house using the information available (i.e. all the columns except for the price). For now, we will only use one feature / variable (sqft_living) to be able to visualize the dataset in a 2D plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# common summary statistics\n",
    "rawData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this gets the columns \"sqft living\" and \"price\"\n",
    "data = rawData[['sqft_living', 'price']]\n",
    "data.head()\n",
    "\n",
    "# convert the pandas dataframe to numpy array\n",
    "data = data.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will separate our dataset into 2 datasets: the **train** and **test** datasets.\n",
    "\n",
    "The linear regression model will look for a good set of **weights/theta** after looking at the train dataset. After training, we will confirm with the never-before-trained-on test set and see if the weights we learned will still perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_data = data.shape[0]\n",
    "\n",
    "# Split the dataset into train, val, and test\n",
    "train_percentage = 0.7\n",
    "num_train = np.floor(num_data*train_percentage).astype(int)\n",
    "num_val = (num_data - num_train) // 2\n",
    "num_test = (num_data - num_train) // 2\n",
    "\n",
    "# numpy.expand_dims inserts a new axis / dimension. \n",
    "# This is makes it more convenient to process later on\n",
    "# doc url: https://docs.scipy.org/doc/numpy/reference/generated/numpy.expand_dims.html\n",
    "X_train = np.expand_dims(data[0:num_train,0], -1)\n",
    "y_train = np.expand_dims(data[0:num_train,1], -1)\n",
    "\n",
    "X_val = np.expand_dims(data[num_train:num_train+num_val,0], -1)\n",
    "y_val = np.expand_dims(data[num_train:num_train+num_val,1], -1)\n",
    "\n",
    "X_test = np.expand_dims(data[num_train+num_val:num_train+num_val+num_test,0], -1)\n",
    "y_test = np.expand_dims(data[num_train+num_val:num_train+num_val+num_test,1], -1)\n",
    "\n",
    "print('Training data shape:',X_train.shape)\n",
    "print('Training ground truth values shape:',y_train.shape)\n",
    "\n",
    "print('Validation data shape:',X_val.shape)\n",
    "print('Validation ground truth values shape:',y_val.shape)\n",
    "\n",
    "print('Test data shape:',X_test.shape)\n",
    "print('Test ground truth values shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the data\n",
    "plt.scatter(X_train,y_train,c='green')\n",
    "plt.ylabel(\"Price\")\n",
    "plt.xlabel(\"Living area\")\n",
    "plt.xlim([0,5000])\n",
    "plt.ylim([0,1500000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can see the (positive) linear relationship between the living area of the house and its price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "$X \\in \\mathbb{R}^{N,D}$ - our data is represented as a matrix with $N$ rows and $D$ columns, where each row is a $D$-dimensional feature vector representing an instance / example in our dataset.\n",
    "\n",
    "$y \\in \\mathbb{R}^N$ - the prediction target is represented as a vector of length $N$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Analytical solution/Closed form/Normal Equation\n",
    "\n",
    "We can quickly compute for the weights by getting the derivative of our objective function and equating it to 0. However, there are some drawbacks to using this method, as you would see in the following cells.\n",
    "\n",
    "The next cells show the step-by-step process of implementing the closed form solution of linear regression:\n",
    "1. Adding $w_0$ in the weight vector $W$ and $x_0$ in $X$ to account for the bias term.\n",
    "1. Finding for the optimal values of weight vector W using the normal equation.\n",
    "3. Do some predictions! Now that we have the weights, the label can be solved by plugging in the features $x$ into the linear model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Pre-processing! Add in a vector of one to X to account for the bias\n",
    "This just appends a vector of ones to the dimension of your feature vector to accomodate for the bias / constant term in our hypothesis function.\n",
    "\n",
    "**Open `linear_regression.py`, and fill in the code for the function `feature_transform`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from linear_regression import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first use a smaller 'dummy' data to make it easier to test and debug your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "np.random.seed(1)\n",
    "dummy_X = np.random.randn(5,3)\n",
    "dummy_transformed_X = regressor.feature_transform(dummy_X)\n",
    "\n",
    "print(\"Before feature transform:\")\n",
    "print(dummy_X)\n",
    "print(\"Shape:\",dummy_X.shape)\n",
    "print()\n",
    "print(\"After feature transform:\")\n",
    "print(dummy_transformed_X)\n",
    "print(\"Shape:\",dummy_transformed_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output of the feature transform function.\n",
    "\n",
    "Before feature transform:\n",
    "```\n",
    "[[ 1.62434536 -0.61175641 -0.52817175]\n",
    " [-1.07296862  0.86540763 -2.3015387 ]\n",
    " [ 1.74481176 -0.7612069   0.3190391 ]\n",
    " [-0.24937038  1.46210794 -2.06014071]\n",
    " [-0.3224172  -0.38405435  1.13376944]]\n",
    "Shape: (5, 3)\n",
    "```\n",
    "After feature transform:\n",
    "\n",
    "```\n",
    "[[ 1.62434536 -0.61175641 -0.52817175  1.        ]\n",
    " [-1.07296862  0.86540763 -2.3015387   1.        ]\n",
    " [ 1.74481176 -0.7612069   0.3190391   1.        ]\n",
    " [-0.24937038  1.46210794 -2.06014071  1.        ]\n",
    " [-0.3224172  -0.38405435  1.13376944  1.        ]]\n",
    "Shape: (5, 4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train! Compute for the weights via closed form/analytical solution\n",
    "**In `linear_regression.py`, fill in the code for the function `train_analytic`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "dummy_X = np.random.randn(5,4)\n",
    "dummy_y = np.random.randn(5,1)\n",
    "regressor.train_analytic(dummy_X,dummy_y)\n",
    "\n",
    "print(\"Weight vector:\")\n",
    "print(regressor.params['W'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected weights $W$.\n",
    "\n",
    "Weight vector:\n",
    "```\n",
    "[[ 3.58229626]\n",
    " [ 4.10375196]\n",
    " [ 4.66577618]\n",
    " [ 4.14835304]\n",
    " [ 2.50637491]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Test! Predict test data with the weights computed\n",
    "**In `linear_regression.py`, fill in the code for the function `predict`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "dummy_X = np.random.randn(5,4)\n",
    "dummy_y = np.random.randn(5,1)\n",
    "regressor.train_analytic(dummy_X,dummy_y)\n",
    "predictions = regressor.predict(dummy_X)\n",
    "\n",
    "print(\"Input\",'\\t\\t\\t\\t\\t\\t\\t','Predictions')\n",
    "for i in range(dummy_X.shape[0]):\n",
    "    print(dummy_X[i],'\\t',predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "\n",
    "```\n",
    "Input \t\t\t\t\t\t\t                     Predictions\n",
    "[ 1.62434536 -0.61175641 -0.52817175 -1.07296862] \t [-1.10061918]\n",
    "[ 0.86540763 -2.3015387   1.74481176 -0.7612069 ] \t [ 1.14472371]\n",
    "[ 0.3190391  -0.24937038  1.46210794 -2.06014071] \t [ 0.90159072]\n",
    "[-0.3224172  -0.38405435  1.13376944 -1.09989127] \t [ 0.50249434]\n",
    "[-0.17242821 -0.87785842  0.04221375  0.58281521] \t [ 0.90085595]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have now successfully implemented the analytical solution for Linear Regression. Let's try it out on actual (non-dummy) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.train_analytic(X_train,y_train)\n",
    "\n",
    "print(\"Weights W:\")\n",
    "print(regressor.params[\"W\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize. It helps to check how our hypothesis \"line\" looks like plotted with our data. Let's visualize using our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot fitted regression line \n",
    "X_range = np.expand_dims(np.arange(0,5000,1),1)\n",
    "y_range = regressor.predict(X_range)\n",
    "\n",
    "plt.scatter(X_train,y_train,c='green')\n",
    "plt.plot(X_range,y_range,c='blue')\n",
    "plt.xlim([0,5000])\n",
    "plt.ylim([0,1500000])\n",
    "plt.ylabel(\"Price\")\n",
    "plt.xlabel(\"Living area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see a line cutting right across the middle of the green data dots, that's a great sign that our model was able to train properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test it with our own input\n",
    "\n",
    "The following input will allow us to estimate a price given a living area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Type in Q to quit.\\n\")\n",
    "while(True):\n",
    "    print(\"Living area:\")\n",
    "    a = input()\n",
    "\n",
    "    if(a == \"Q\" or a == 'q'):\n",
    "        break;\n",
    "    elif(a.isdigit()):\n",
    "        X_in = np.expand_dims([int(a)],1)\n",
    "        y_in = regressor.predict(X_in)\n",
    "        print(\"For a living area of\", a , \"sqm, the estimated price is $%0.2f \\n\" % np.squeeze(y_in))\n",
    "    else:\n",
    "        print(\"Please enter a valid number.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this out.** Try putting in a small number for our square footage. Did you see a negative price? \n",
    "\n",
    "Right now, the model predicts negative prices for small inputs. This is because our model does not know that negative prices do not make sense, it will just fit a linear function that extends to the whole range of real numbers including negative numbers. In practice you might want to perform some post processing or some other work arounds to make the predictions more reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical solution method in a nutshell\n",
    "The analytical solution / normal equation directly solves for the optimal parameters. This will work well for some problems, but it requires taking the inverse of the matrix $X^TX$. The time complexity of an inverse operation is approximately $O(n^3)$. We also need to store all of our data $X$ in memory to be able to compute for $X^TX$. This makes it impractical to use for very large / high-dimensional datasets. \n",
    "\n",
    "Another way to solve for the parameters is using stochastic gradient descent which we will implement next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 : Iterative solution using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells show the step-by-step process of implementing the stochastic gradient descent solution of linear regression:\n",
    "\n",
    "1. Adding $w_0$ in the weight vector $W$ and $x_0$ in $X$ to account for the bias term.\n",
    "2. Initialize the parameters/weight vector $W$\n",
    "3. Calculate the cost/loss/objective function for the current parameters/weights\n",
    "4. Compute for the gradients of the loss function with respect to the parameters/weights\n",
    "5. Update the paramters/weights based on the computed gradients.\n",
    "\n",
    "Like before, we will first test and debug our code using a smaller dummy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: (Done) Pre-processing! Add in a vector of one to X to account for the bias\n",
    "We have already implemented this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initialize Weights! Initialize the weights to a random value.\n",
    "\n",
    "\n",
    "**Open `linear_regression.py`, and fill in the code for the function `initialize_weights`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.initialize_weights(5)\n",
    "print(\"Weights vector:\")\n",
    "print(regressor.params[\"W\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Weights vector:\n",
    "[[ 0.01624345]\n",
    " [-0.00611756]\n",
    " [-0.00528172]\n",
    " [-0.01072969]\n",
    " [ 0.00865408]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 and 4: Compute for the loss and its gradients with respect to the current weights.\n",
    "\n",
    "**Open `linear_regression.py`, and fill in the code for the function `loss`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "dummy_X = np.random.randn(5,5)\n",
    "dummy_y = np.random.randn(5,1)\n",
    "regressor.initialize_weights(5)\n",
    "loss, grads = regressor.loss(dummy_X, dummy_y)\n",
    "print(\"Loss :\", loss)\n",
    "print(\"grads['W'] :\",grads['W'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Loss : 0.1724358913\n",
    "grads['W'] : [[ 0.48985334]\n",
    " [-0.55388664]\n",
    " [-0.29967106]\n",
    " [-0.26149345]\n",
    " [ 0.25690887]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Iteratively update the weights based on the gradient.\n",
    "\n",
    "**Open `linear_regression.py`, and fill in the code for the function `train`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "np.random.seed(1)\n",
    "dummy_X = np.random.randn(1000,5)\n",
    "dummy_y = np.sum(dummy_X*3,axis=1, keepdims=True) + np.random.randn(1000,1)*0.5\n",
    "stats = regressor.train(dummy_X,dummy_y,learning_rate=0.01,num_iters=500,batch_size=50,verbose=True)\n",
    "plt.plot(stats['loss_history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "The loss plot should be exponentially decreasing and the loss values should be similar to the one below.\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Epoch 0 \t training RMSE: 6.6090\n",
    "Epoch 1 \t training RMSE: 5.3775\n",
    "Epoch 2 \t training RMSE: 5.0075\n",
    "Epoch 3 \t training RMSE: 3.7524\n",
    "Epoch 4 \t training RMSE: 3.6866\n",
    "iteration 100 / 500: loss 3.766868\n",
    "Epoch 5 \t training RMSE: 2.2935\n",
    "Epoch 6 \t training RMSE: 2.2515\n",
    "Epoch 7 \t training RMSE: 1.7601\n",
    "Epoch 8 \t training RMSE: 1.7724\n",
    "Epoch 9 \t training RMSE: 1.9090\n",
    "iteration 200 / 500: loss 1.001868\n",
    "Epoch 10 \t training RMSE: 1.6331\n",
    "Epoch 11 \t training RMSE: 1.1228\n",
    "Epoch 12 \t training RMSE: 1.3707\n",
    "Epoch 13 \t training RMSE: 1.0007\n",
    "Epoch 14 \t training RMSE: 1.4122\n",
    "iteration 300 / 500: loss 0.486848\n",
    "Epoch 15 \t training RMSE: 1.0572\n",
    "Epoch 16 \t training RMSE: 0.8985\n",
    "Epoch 17 \t training RMSE: 0.7991\n",
    "Epoch 18 \t training RMSE: 0.7868\n",
    "Epoch 19 \t training RMSE: 0.7851\n",
    "iteration 400 / 500: loss 0.236503\n",
    "Epoch 20 \t training RMSE: 0.7128\n",
    "Epoch 21 \t training RMSE: 0.7702\n",
    "Epoch 22 \t training RMSE: 0.7366\n",
    "Epoch 23 \t training RMSE: 0.7527\n",
    "Epoch 24 \t training RMSE: 0.5996\n",
    "iteration 500 / 500: loss 0.154280\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have now successfully implemented Linear Regression using Stochastic Gradient Descent. Let's try it out on actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "regressor = LinearRegression()\n",
    "stats = regressor.train(X_train,y_train,learning_rate=1e-4,num_iters=500,batch_size=512,verbose=True)\n",
    "plt.plot(stats['loss_history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things not going as expected?** If the magnitude of the output and feature values are quite big, we can easily encounter numerical overflow (or underflow if the magnitude is too small). To address this issue, it is often a good idea to normalize or standardize the data.\n",
    "\n",
    "### (Pre-processing) Standardize the features by subtracting it with its mean and then dividing by its standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Standardize the training data and the validation data.                       #\n",
    "################################################################################\n",
    "\n",
    "X_train_norm = None\n",
    "y_train_norm = None\n",
    "\n",
    "X_val_norm = None\n",
    "y_val_norm = None\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "regressor = LinearRegression()\n",
    "stats = regressor.train(X_train_norm,y_train_norm,X_val_norm, y_val_norm,\n",
    "                            learning_rate=1e-2,learning_rate_decay=0.9,num_iters=1000,batch_size=512,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check**\n",
    "```\n",
    "Epoch 0 \t training RMSE: 0.9630\n",
    "\t\t validation RMSE: 0.9605\n",
    "Epoch 1 \t training RMSE: 0.8200\n",
    "\t\t validation RMSE: 0.8651\n",
    "Epoch 2 \t training RMSE: 0.8288\n",
    "\t\t validation RMSE: 0.8060\n",
    "Epoch 3 \t training RMSE: 1.0526\n",
    "\t\t validation RMSE: 0.7706\n",
    "iteration 100 / 1000: loss 0.319256\n",
    "Epoch 4 \t training RMSE: 0.7171\n",
    "\t\t validation RMSE: 0.7453\n",
    "Epoch 5 \t training RMSE: 0.8004\n",
    "\t\t validation RMSE: 0.7308\n",
    "Epoch 6 \t training RMSE: 0.7045\n",
    "\t\t validation RMSE: 0.7204\n",
    "iteration 200 / 1000: loss 0.354085\n",
    "Epoch 7 \t training RMSE: 0.7163\n",
    "\t\t validation RMSE: 0.7136\n",
    "Epoch 8 \t training RMSE: 0.8864\n",
    "\t\t validation RMSE: 0.7091\n",
    "Epoch 9 \t training RMSE: 0.7350\n",
    "\t\t validation RMSE: 0.7050\n",
    "Epoch 10 \t training RMSE: 0.7186\n",
    "\t\t validation RMSE: 0.7037\n",
    "iteration 300 / 1000: loss 0.195399\n",
    "Epoch 11 \t training RMSE: 0.6950\n",
    "\t\t validation RMSE: 0.7017\n",
    "Epoch 12 \t training RMSE: 0.7377\n",
    "\t\t validation RMSE: 0.7007\n",
    "Epoch 13 \t training RMSE: 0.7426\n",
    "\t\t validation RMSE: 0.6996\n",
    "iteration 400 / 1000: loss 0.275122\n",
    "Epoch 14 \t training RMSE: 0.8269\n",
    "\t\t validation RMSE: 0.6986\n",
    "Epoch 15 \t training RMSE: 0.6815\n",
    "\t\t validation RMSE: 0.6979\n",
    "Epoch 16 \t training RMSE: 0.6626\n",
    "\t\t validation RMSE: 0.6973\n",
    "Epoch 17 \t training RMSE: 0.8149\n",
    "\t\t validation RMSE: 0.6969\n",
    "iteration 500 / 1000: loss 0.284460\n",
    "Epoch 18 \t training RMSE: 0.7000\n",
    "\t\t validation RMSE: 0.6966\n",
    "Epoch 19 \t training RMSE: 0.8664\n",
    "\t\t validation RMSE: 0.6963\n",
    "Epoch 20 \t training RMSE: 0.6585\n",
    "\t\t validation RMSE: 0.6961\n",
    "iteration 600 / 1000: loss 0.219908\n",
    "Epoch 21 \t training RMSE: 0.6556\n",
    "\t\t validation RMSE: 0.6960\n",
    "Epoch 22 \t training RMSE: 0.6476\n",
    "\t\t validation RMSE: 0.6958\n",
    "Epoch 23 \t training RMSE: 0.6376\n",
    "\t\t validation RMSE: 0.6957\n",
    "Epoch 24 \t training RMSE: 0.8272\n",
    "\t\t validation RMSE: 0.6956\n",
    "iteration 700 / 1000: loss 0.179505\n",
    "Epoch 25 \t training RMSE: 0.7812\n",
    "\t\t validation RMSE: 0.6955\n",
    "Epoch 26 \t training RMSE: 0.7189\n",
    "\t\t validation RMSE: 0.6954\n",
    "Epoch 27 \t training RMSE: 0.6691\n",
    "\t\t validation RMSE: 0.6953\n",
    "iteration 800 / 1000: loss 0.233584\n",
    "Epoch 28 \t training RMSE: 0.8886\n",
    "\t\t validation RMSE: 0.6952\n",
    "Epoch 29 \t training RMSE: 0.6723\n",
    "\t\t validation RMSE: 0.6952\n",
    "Epoch 30 \t training RMSE: 0.7169\n",
    "\t\t validation RMSE: 0.6951\n",
    "iteration 900 / 1000: loss 0.352268\n",
    "Epoch 31 \t training RMSE: 0.8394\n",
    "\t\t validation RMSE: 0.6951\n",
    "Epoch 32 \t training RMSE: 0.6996\n",
    "\t\t validation RMSE: 0.6951\n",
    "Epoch 33 \t training RMSE: 0.7227\n",
    "\t\t validation RMSE: 0.6950\n",
    "Epoch 34 \t training RMSE: 0.6987\n",
    "\t\t validation RMSE: 0.6950\n",
    "iteration 1000 / 1000: loss 0.371101\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are your results slightly different? You might want to consider checking your code again. \n",
    "\n",
    "**Hint**: it may have to do with how you treat your data before predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to keep track of the losses as well as both the training set performance and the validation set performance while training so we can get some insights as to how the learning process is going. We will be discussing more on error analysis later on in the course. Let's visualize the loss history as well as the RMSE history of the training set and validation set during training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "tr = plt.plot(stats['train_rmse'], label='train')\n",
    "vl = plt.plot(stats['val_rmse'], label='val')\n",
    "plt.title('Root Mean Squared Error history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "plt.legend(('val','train'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of them seem to be converging, which is usually a good sign.\n",
    "\n",
    "Now that we have trained our model, we need to convert the predicted values back to its original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot fitted regression line \n",
    "X_range = np.expand_dims(np.arange(0,5000,1),1)\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Standardize the data.                                                        #\n",
    "################################################################################\n",
    "X_range_norm = None\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "y_range_norm = regressor.predict(X_range_norm)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Convert the predicted y values back to its original scale.                   #\n",
    "################################################################################\n",
    "y_range = None\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "plt.scatter(X_train,y_train,c='green')\n",
    "plt.plot(X_range,y_range,c='blue')\n",
    "plt.xlim([0,5000])\n",
    "plt.ylim([0,1500000])\n",
    "plt.ylabel(\"Price\")\n",
    "plt.xlabel(\"Living area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now try the model on the separate test set in order to fairly evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot fitted regression line \n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Standardize the your training and validation data by subtracting it with     #\n",
    "# its mean and then dividing by its standard deviation.                        #\n",
    "################################################################################\n",
    "X_test_norm = None\n",
    "y_test_norm = None\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "y_test_norm_pred = regressor.predict(X_test_norm)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Convert the predicted y values back to its original scale.                   #\n",
    "################################################################################\n",
    "y_test_pred = None\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(X_test,y_test,c='green')\n",
    "plt.plot(X_test,y_test_pred,c='blue')\n",
    "plt.xlim([0,5000])\n",
    "plt.ylim([0,1500000])\n",
    "plt.ylabel(\"Price\")\n",
    "plt.xlabel(\"Living area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(np.mean((y_test_norm_pred - y_test_norm)**2))\n",
    "print(\"Test set RMSE:\",RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the learning rate affect the training?\n",
    "In practice, we do not know the optimal learning rate in advanced. One way to search for a good learning rate is to try different values of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "alphas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "plt_ctr = 1\n",
    "plt.figure(figsize=(15,10))\n",
    "for alpha in alphas:\n",
    "    regressor = LinearRegression()\n",
    "    stats = regressor.train(X_train_norm,y_train_norm,X_val_norm, y_val_norm,\n",
    "                                learning_rate=alpha,learning_rate_decay=0.9,num_iters=1000,batch_size=512,verbose=False)\n",
    "    plt.subplot(2,3,plt_ctr)\n",
    "    plt.plot(stats['loss_history'])\n",
    "    plt.title('Loss history (alpha = '+str(alpha)+')')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt_ctr+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the shape of the loss plot the optimal learning rate would be near 0.01. Later on in the course we will be discussing how to efficiently tune hyperparameters such as the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment!\n",
    "We have only used one feature so far. Try out other feature combinations to improve the model's performance. Feel free to try out other things as well. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> fin </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
