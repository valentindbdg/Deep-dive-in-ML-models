{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multinomial Logistic Regression (SoftMax) Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This exercise will guide you in extending the Binary Logistic Regression Model to a Multinomial Logistic Regression model that can handle more than 2 classes. It is more commonly refered to as the SoftMax regression.\n",
    "\n",
    "**You will learn to:**\n",
    "- Build the general architecture of a Multinomial Logistic Regression (SoftMax) Model.\n",
    "    - Initializing Parameters/Weights\n",
    "    - Implement the activation function that maps your raw scores to a probability distribution over all classes.\n",
    "    - Calculating the Cost/Loss/Objective Function\n",
    "    - Computing for the gradients of the Loss function with respect to the parameters\n",
    "    - Implement gradient descent to update the paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Fix the seed of the random number \n",
    "# generator so that your results will match ours\n",
    "np.random.seed(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data\n",
    "As with the logistic regression exercise, we will use a toy dataset, so we can visualize our data and model predictions in 2D. The data generated below are sampled from three Gaussian distributions centered at $(-5, 0)$, $(0, 2)$ and $(-1, -4)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "from sklearn.datasets import make_blobs\n",
    "centers = [[-5, 0], [0, 2], [-1, -4]]\n",
    "X, y = make_blobs(n_samples=1000, centers=centers)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1],c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "$X \\in \\mathbb{R}^{N,D}$ - like the binary logistic regression, our data is also represented as a matrix with $N$ rows and $D$ columns, where each row is a $D$-dimensional feature vector representing an instance / example in our dataset $(x_i \\in \\mathbb{R}^D)$. In this particular example, $D=2$.\n",
    "\n",
    "\n",
    "\n",
    "$y \\in \\{0,..,C\\}^N$ - Given $C$ distinct classes, the prediction target is represented as a vector of length $N$ and each example $y_i$ is a scalar that can take on a value from 0 to $C$.\n",
    "\n",
    "**Note that the math expresses our target variable $y_i$ as a one-hot encoding vector, where it has a value of 1 corresponding to the correct class and 0 everywhere else. In practice, we represent $y_i$ as a scalar value denoting the index of the correct class instead. This is because it is not computationally and memory efficient to treat each $y_i$ as a vector, specially for large number of classes, when almost all of its value are 0.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"The shape of X:\", X.shape)\n",
    "print(\"The shape of y:\", y.shape)\n",
    "print(\"\\nFirst 5 examples:\")\n",
    "for i in range(5):\n",
    "    print(\"X[{}] = {}\\t y[{}] = {}\".format(i, X[i], i, y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multinomial Logistic Regression (SoftMax Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  Initialize Weights! We initialize the weights with small random values and the biases are initialized to zero.\n",
    "**Open `multinomial_logistic_regression.py`, and fill in the code for the function `initialize_weights`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from multinomial_logistic_regression import MultinomialLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "classifier = MultinomialLogisticRegression()\n",
    "classifier.initialize_weights(5,5)\n",
    "print(\"Weights vector:\")\n",
    "print(classifier.params[\"W\"])\n",
    "print(\"Bias:\")\n",
    "print(classifier.params[\"b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Weights vector:\n",
    "[[ 0.01624345 -0.00611756 -0.00528172 -0.01072969  0.00865408]\n",
    " [-0.02301539  0.01744812 -0.00761207  0.00319039 -0.0024937 ]\n",
    " [ 0.01462108 -0.02060141 -0.00322417 -0.00384054  0.01133769]\n",
    " [-0.01099891 -0.00172428 -0.00877858  0.00042214  0.00582815]\n",
    " [-0.01100619  0.01144724  0.00901591  0.00502494  0.00900856]]\n",
    "Bias:\n",
    "[ 0.  0.  0.  0.  0.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Compute for the predictions using the current weights\n",
    "**First let's implement the `softmax` function that converts our scores to probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "classifier = MultinomialLogisticRegression()\n",
    "probs = classifier.softmax(np.random.randn(3,5))\n",
    "print(\"Probabilities of belonging to each class\")\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Probabilities of belonging to each class\n",
    "[[ 0.56862917  0.06077185  0.06606977  0.0383178   0.26621141]\n",
    " [ 0.01185074  0.67772435  0.05529718  0.16287255  0.09225519]\n",
    " [ 0.48184163  0.01423021  0.08089001  0.07605473  0.34698342]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's test your implementation again on a different scale of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "classifier = MultinomialLogisticRegression()\n",
    "probs = classifier.softmax(np.array([[1001,1002,1003,1004,1005],[3,4,5,6,7]]))\n",
    "print(\"Probabilities of belonging to each class\")\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Probabilities of belonging to each class\n",
    "[[ 0.01165623  0.03168492  0.08612854  0.23412166  0.63640865]\n",
    " [ 0.01165623  0.03168492  0.08612854  0.23412166  0.63640865]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Are you getting numerical overflow errors or `nan` values?\n",
    "\n",
    "This is because of exponentiating large values can easily lead to numerical overflows so in practice softmax is implemented a little bit differently.\n",
    "\n",
    "First, you must show mathematically that softmax is invariant to constant offsets in the input. More specifically, given any input vector $x$ and any constant $c$,\n",
    "\n",
    "$$\\text{softmax}(x) = \\text{softmax}(x+c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#################################################################################\n",
    "\n",
    "**Double click this cell and type your answer below:**\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{softmax}(x)_i &= \\frac{e^{x_i}}{\\sum_j e^{x_j}} \\\\\n",
    "    &= ? \\\\\n",
    "\\end{align}\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "**END**\n",
    "\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In implementing the softmax function, we use this property and choose $c = -\\max(x)$ to make it more numerically stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**We can now implment the `predict` function, which takes in an input vector and outputs index of the correct class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "classifier = MultinomialLogisticRegression()\n",
    "classifier.initialize_weights(5,3)\n",
    "\n",
    "print(\"Predictions\")\n",
    "print(classifier.predict(np.random.randn(10,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Predictions\n",
    "[2 1 2 2 1 1 0 2 0 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Compute for the losses corresponding to the current predictions.\n",
    "**Implement `loss` function which should output the losses as well as its the gradients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "classifier = MultinomialLogisticRegression()\n",
    "classifier.initialize_weights(5,3)\n",
    "dummy_labels = [0,1,2,1,0]\n",
    "loss, grads = classifier.loss(np.random.randn(5,5),dummy_labels)\n",
    "\n",
    "print(\"Loss:\",loss)\n",
    "print(\"Gradient['W']\",grads['W'])\n",
    "print(\"Gradient['b']\",grads['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Loss: 1.09729511651\n",
    "Gradient['W'] [[-0.01192344  0.11655022 -0.10462678]\n",
    " [ 0.21354341 -0.19261624 -0.02092716]\n",
    " [ 0.0410376  -0.13354762  0.09251001]\n",
    " [-0.26650623  0.14255741  0.12394882]\n",
    " [-0.12784662  0.09548254  0.03236408]]\n",
    "Gradient['b'] [-0.07185451 -0.06396272  0.13581723]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Lastly, we use gradient descent algorithm in order to train our model.\n",
    "**Implement gradient descent in the `train` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "classifier = MultinomialLogisticRegression()\n",
    "loss_history = classifier.train(X, y, learning_rate=0.9, lambda_reg=0.0, num_iters=2000, batch_size=256, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Sanity Check:**\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "iteration 100 / 2000: loss 0.013159\n",
    "iteration 200 / 2000: loss 0.016051\n",
    "iteration 300 / 2000: loss 0.019418\n",
    "iteration 400 / 2000: loss 0.011900\n",
    "iteration 500 / 2000: loss 0.006884\n",
    "iteration 600 / 2000: loss 0.021871\n",
    "iteration 700 / 2000: loss 0.029994\n",
    "iteration 800 / 2000: loss 0.009977\n",
    "iteration 900 / 2000: loss 0.015705\n",
    "iteration 1000 / 2000: loss 0.012317\n",
    "iteration 1100 / 2000: loss 0.024872\n",
    "iteration 1200 / 2000: loss 0.015629\n",
    "iteration 1300 / 2000: loss 0.005623\n",
    "iteration 1400 / 2000: loss 0.006077\n",
    "iteration 1500 / 2000: loss 0.021168\n",
    "iteration 1600 / 2000: loss 0.019760\n",
    "iteration 1700 / 2000: loss 0.020992\n",
    "iteration 1800 / 2000: loss 0.008539\n",
    "iteration 1900 / 2000: loss 0.013216\n",
    "iteration 2000 / 2000: loss 0.006798\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_train_pred = classifier.predict(X)\n",
    "print(\"Train accuracy: {} %\".format(np.mean((Y_train_pred == y)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Let's visualize the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "\n",
    "x_test = np.squeeze(np.stack((xx.ravel(),yy.ravel()))).T\n",
    "Z = classifier.predict(x_test)\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Let's add $L_2$ regularization and see what happens to the decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "classifier = MultinomialLogisticRegression()\n",
    "loss_history = classifier.train(X, y, learning_rate=0.9, lambda_reg=0.1, num_iters=2000, batch_size=256, verbose=False)\n",
    "\n",
    "x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "\n",
    "x_test = np.squeeze(np.stack((xx.ravel(),yy.ravel()))).T\n",
    "Z = classifier.predict(x_test)\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='black')\n",
    "\n",
    "Y_train_pred = classifier.predict(X)\n",
    "print(\"Train accuracy: {} %\".format(np.mean((Y_train_pred == y)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can observe that the training accuracy went down a little bit and the decision boundaries have more equal angles across the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
